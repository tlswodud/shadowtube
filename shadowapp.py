import streamlit as st
##en-core-web-sm==3.8.0
import yt_dlp

def get_video_info(url):
    try:
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': True
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            return {
                'title': info.get('title'),
                'channel': info.get('uploader'),
                'thumbnail': info.get('thumbnail'),
                'duration': info.get('duration'),
                'view_count': info.get('view_count')
            }
    except Exception as e:
        st.error(f"ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None
    
def create_modern_ui():
    # í—¤ë” ì„¹ì…˜
    st.title("ğŸ¯ Shadowing YouTube Subtitle Generator!")
    
    # ì†Œê°œ ì„¹ì…˜
    with st.container():
        st.markdown("""
        ### ğŸš€ Learn Languages Through YouTube!
        Transform your favorite YouTube videos into powerful learning materials.
        """)
        
        # êµ¬ë¶„ ì¶”ê°€
        st.divider()
    
    # ì–¸ì–´ ì„ íƒ ì„¹ì…˜ - 2ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ‘¤ Native Language")
        native_language = st.selectbox(
            "Select your native language",
            ["í•œêµ­ì–´", "English", "æ—¥æœ¬èª", "ä¸­æ–‡", "EspaÃ±ol", "FranÃ§ais"],
            label_visibility="collapsed"
        )
        
    with col2:
        st.markdown("### ğŸ¯ Target Language")
        target_language = st.selectbox(
            "Select language you want to learn",
            ["English", "í•œêµ­ì–´", "æ—¥æœ¬èª", "ä¸­æ–‡", "EspaÃ±ol", "FranÃ§ais"],
            label_visibility="collapsed"
        )
    
    # ì„ íƒëœ ì–¸ì–´ ì •ë³´ í‘œì‹œ
    with st.container():
        st.info(f"""
        ğŸ“ Selected Configuration:
        - Native Language: {native_language} (code: {get_language_code(native_language)})
        - Target Language: {target_language} (code: {get_language_code(target_language)})
        """)
    
   

    # URL ì…ë ¥ ì„¹ì…˜
    st.markdown("### ğŸ¥ YouTube Video")
    url = st.text_input(
        "Enter YouTube URL",
        placeholder="https://www.youtube.com/watch?v=...",
        label_visibility="collapsed"
    )
    
    # ë¹„ë””ì˜¤ ì •ë³´ í‘œì‹œ
    if url:
        video_info = get_video_info(url)
        if video_info:
            with st.container():
                col1, col2 = st.columns([1, 3])
                with col1:
                    st.image(video_info['thumbnail'], width=160)
                with col2:
                    st.markdown(f"#### {video_info['title']}")
                    st.caption(f"ğŸ“º {video_info['channel']}")
                    # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                    duration_min = video_info['duration'] // 60
                    duration_sec = video_info['duration'] % 60
                    st.caption(f"â±ï¸ {duration_min}:{duration_sec:02d} | ğŸ‘€ {video_info['view_count']:,} views")
        
    # ì²˜ë¦¬ ë²„íŠ¼
    if st.button("ğŸ¯ Generate Shadowing Materials", type="primary", use_container_width=True):
        with st.spinner("Processing your request..."):
            # ì²˜ë¦¬ ë¡œì§
            pass

    # ë„ì›€ë§ ì„¹ì…˜
    with st.expander("â„¹ï¸ How to use"):
        st.markdown("""
        1. Paste your free Gemini API Key.            
        2. Select your native language
        3. Choose the language you want to learn
        4. Paste a YouTube URL
        5. Click 'Generate' to create your learning materials
        """)
    return url ,  native_language  ,target_language

def get_language_code(language):
    # ì–¸ì–´ ì½”ë“œ ë§¤í•‘
    codes = {
        "í•œêµ­ì–´": "ko",
        "English": "en",
        "æ—¥æœ¬èª": "ja",
        "ä¸­æ–‡": "zh-Hans",
        "EspaÃ±ol": "es",
        "FranÃ§ais": "fr"
    }
    return codes.get(language, "unknown")

st.set_page_config(
    page_title="ShadowTube",
    page_icon="â–¶ï¸",
    layout="wide"
)
# ìŠ¤íƒ€ì¼ë§
st.markdown("""
    <style>
    .stButton>button {
        font-size: 1.2rem;
        margin-top: 2rem;
        margin-bottom: 2rem;
    }
    .stSelectbox {
        margin-bottom: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)

# UI ì‹¤í–‰
url , native_language  ,want_language = create_modern_ui()

native_code=get_language_code(native_language)

learn_code = get_language_code(want_language)

st.markdown("""
    <style>
    .chat-message {
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
    }
    .chat-message.user {
        background-color: #f0f2f6;
    }
    .chat-message.assistant {
        background-color: white;
        border: 1px solid #e0e0e0;
    }
    .chat-icon {
        width: 40px;
        margin-right: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def display_chat_message(role, content):
    icon = "ğŸ‘¤" if role == "user" else "ğŸ¤–"
    st.markdown(f"""
        <div class="chat-message {role}">
            <div class="chat-icon">{icon}</div>
            <div>{content}</div>
        </div>
    """, unsafe_allow_html=True)

import streamlit as st
from io import BytesIO
from docx import Document

import streamlit as st
from io import BytesIO
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn
from docx.oxml import OxmlElement

from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn
from docx.oxml import OxmlElement
from io import BytesIO
from urllib.parse import urlparse, parse_qs
import yt_dlp
from youtube_transcript_api import YouTubeTranscriptApi

from langdetect import detect
from docx import Document
from docx.shared import Pt
from io import BytesIO
from lxml import etree
from docx.oxml import OxmlElement



def create_word_file_shadow_script(content, utb_title, learn_code, want_font, native_font, font_size): 
    """
    ê° ì–¸ì–´ì— ë§ëŠ” í°íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ ì›Œë“œ íŒŒì¼ ìƒì„±
    content: í…ìŠ¤íŠ¸ ë‚´ìš©
    utb_title: ìœ íŠœë¸Œ ì œëª©
    learn_code: í•™ìŠµí•˜ê³ ì í•˜ëŠ” ì–¸ì–´ ì½”ë“œ
    native_font: ëª¨êµ­ì–´ í°íŠ¸
    want_font: í•™ìŠµ ì–¸ì–´ í°íŠ¸
    font_size: í°íŠ¸ í¬ê¸°
    """
    # ì›Œë“œ ë¬¸ì„œ ê°ì²´ ìƒì„±
    doc = Document()
    doc.add_heading(f'{utb_title} YouTube Transcript', level=1)

    # ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì„¤ì •
    language_ranges = {
        "zh": [(0x4E00, 0x9FFF)],  # CJK í†µí•© í•œì
        "ko": [(0xAC00, 0xD7AF)],  # í•œê¸€
        "ja": [(0x3040, 0x309F), (0x30A0, 0x30FF)],  # íˆë¼ê°€ë‚˜ + ê°€íƒ€ì¹´ë‚˜
        "en": [(0x0041, 0x005A), (0x0061, 0x007A)],  # ì˜ì–´ ëŒ€ë¬¸ì + ì†Œë¬¸ì
        "fr": [(0x00C0, 0x017F)],  # í”„ë‘ìŠ¤ì–´
        "es": [(0x00C0, 0x017F)]   # ìŠ¤í˜ì¸ì–´
    }

    def is_in_range(char, ranges):
        """ì£¼ì–´ì§„ ë¬¸ì(char)ê°€ íŠ¹ì • ì–¸ì–´ ë²”ìœ„(ranges)ì— ì†í•˜ëŠ”ì§€ í™•ì¸"""
        code = ord(char)
        return any(start <= code <= end for start, end in ranges)

    def get_dominant_language(text):
        """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ì–¸ì–´ë¥¼ ë°˜í™˜"""
        counts = {lang: 0 for lang in language_ranges.keys()}
        for char in text:
            for lang, ranges in language_ranges.items():
                if is_in_range(char, ranges):
                    counts[lang] += 1
                    break
        return max(counts, key=counts.get)  # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ì–¸ì–´ ë°˜í™˜

    # ê° ì¤„ì— ëŒ€í•´ ì²˜ë¦¬
    for line in content:
        paragraph = doc.add_paragraph()
        run = paragraph.add_run(line)
    

        # ì£¼ìš” ì–¸ì–´ í™•ì¸
        dominant_language = get_dominant_language(line)

        # í•™ìŠµ ì–¸ì–´ì™€ ë™ì¼í•˜ë©´ í•™ìŠµ í°íŠ¸, ì•„ë‹ˆë©´ ëª¨êµ­ì–´ í°íŠ¸ ì ìš©
        
        if dominant_language == learn_code:
             font_name = want_font
        else:
             font_name = native_font     

        # í°íŠ¸ í¬ê¸°ì™€ í°íŠ¸ ì„¤ì •
        run.font.size = Pt(font_size)
        rpr = run._element.get_or_add_rPr()
        rFonts = OxmlElement("w:rFonts")

        rFonts.set(qn("w:ascii"), font_name)
        rFonts.set(qn("w:hAnsi"), font_name)

        # CJK ë¬¸ììš© í°íŠ¸ ì„¤ì • (ë™ì•„ì‹œì•„ í°íŠ¸)
        if dominant_language in ["zh", "ko", "ja"]:
            rFonts.set(qn("w:eastAsia"), font_name)

        rpr.append(rFonts)

    # BytesIOë¥¼ ì‚¬ìš©í•´ ì›Œë“œ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    return buffer
# ì‚¬ìš©ì ì…ë ¥
import streamlit as st
@st.cache_data
def get_best_english_transcript(video_id):
        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        english_codes = ['en-US', 'en-GB', 'en-CA', 'en-AU', 'en', 'a.en']
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                
                transcript = transcript_list.find_manually_created_transcript(english_codes)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                result = [(entry['start'], entry['text']) for entry in transcript.fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(english_codes)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                    result = [(entry['start'], entry['text']) for entry in transcript.fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('en'):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                        result = [(entry['start'], entry['text']) for entry in transcript.fetch()]
                        return result
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
@st.cache_data        
def get_best_english_encode(video_id):
        

        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        english_codes = ['en-US', 'en-GB', 'en-CA', 'en-AU', 'en', 'a.en']
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                transcript = transcript_list.find_manually_created_transcript(english_codes)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                return english_codes
                
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(english_codes)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                    return english_codes
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('en'):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                            return english_codes
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
#fetch ì•„ í˜ë“¤ë‹¤..
@st.cache_data
def get_best_english_transcript_no_time(video_id):
        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        english_codes = ['en-US', 'en-GB', 'en-CA', 'en-AU', 'en', 'a.en']
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ï¿½ï¿½ë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                
                transcript = transcript_list.find_manually_created_transcript(english_codes)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                result = [(entry['text']) for entry in transcript.fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(english_codes)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                    result = [(entry['text']) for entry in transcript.fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('en'):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                        result = [(entry['text']) for entry in transcript.fetch()]
                        return result
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
def get_best_english_trans_Ko(video_id):
    
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            english_codes = get_best_english_encode(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                transcript = transcript_list.find_manually_created_transcript(english_codes)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                result = [(entry['text']) for entry in transcript.translate('ko').fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(english_codes)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                    result = [(entry['text']) for entry in transcript.translate('ko').fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('en'):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                        result = [(entry['text']) for entry in transcript.translate('ko').fetch()]
                        return result
                    
                    print("í•œê¸€ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
@st.cache_data
def get_best_ko_transcript_no_time(video_id):
        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        ko_codes = ['ko' , 'a.ko']
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                
                transcript = transcript_list.find_manually_created_transcript(ko_codes)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                result = [(entry['text']) for entry in transcript.fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(ko_codes)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                    result = [(entry['text']) for entry in transcript.fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('ko'):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {transcript.language_code})")
                        result = [(entry['text']) for entry in transcript.fetch()]
                        return result
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None

@st.cache_data
def get_video_id(url):
    """URLì—ì„œ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ"""
    parsed_url = urlparse(url)
    
    # ì¼ë°˜ ìœ íŠœë¸Œ URL (https://www.youtube.com/watch?v=ë¹„ë””ì˜¤ID)
    if parsed_url.hostname == 'www.youtube.com' and parsed_url.path == '/watch':
        return parse_qs(parsed_url.query).get('v', [None])[0]
    
    # ì§§ì€ URL í˜•ì‹ (https://youtu.be/ë¹„ë””ì˜¤ID)
    if parsed_url.hostname == 'youtu.be':
        return parsed_url.path[1:]
    
    # ì„ë² ë“œ URL í˜•ì‹ (https://www.youtube.com/embed/ë¹„ë””ì˜¤ID)
    if parsed_url.hostname == 'www.youtube.com' and parsed_url.path.startswith('/embed/'):
        return parsed_url.path.split('/')[2]
    
    # ìœ íŠœë¸Œ ì‡¼ì¸  URL í˜•ì‹ (https://www.youtube.com/shorts/ë¹„ë””ì˜¤ID)
    if parsed_url.hostname == 'www.youtube.com' and parsed_url.path.startswith('/shorts/'):
        return parsed_url.path.split('/')[2]
    
    return None
# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°

def get_video_title(video_id):
    # yt-dlp ê°ì²´ ìƒì„±
    if video_id == None:
         return None

    ydl_opts = {}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
        info_dict = ydl.extract_info(video_id, download=False)
        # ì œëª© ê°€ì ¸ì˜¤ê¸°
        title = info_dict.get('title', None)
        return title

#ì—¬ê¸°ì„œ ë¶€í„°ëŠ” ì˜ì–´ í•œêµ­ì–´ ë§ê³  ë‹¤ë¥¸ê±°ë„ ê°€ëŠ¥í•˜ê²Œ ë°”ê¿”ì£¼ë ¤ê³  Ko ë„ ë°”ê¿”ì•¼í•¨
@st.cache_data
def get_best_want_no_time(video_id ,learn_code):
        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:      
                transcript = transcript_list.find_manually_created_transcript(learn_code)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ë°°ìš¸ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {learn_code})")
                result = [(entry['text']) for entry in transcript.fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(learn_code)
                    print(f"ìë™ ìƒì„±ëœ ë°°ìš¸ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {learn_code})")
                    result = [(entry['text']) for entry in transcript.fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith(learn_code):
                            print(f"ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {learn_code})")
                        result = [(entry['text']) for entry in transcript.fetch()]
                        return result
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
@st.cache_data
def get_best_to_translate_target(video_id , learn_code):
    
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                transcript = transcript_list.find_manually_created_transcript(learn_code)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {learn_code})")
                result = [(entry['text']) for entry in transcript.translate(learn_code).fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(learn_code)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.  {learn_code}")
                    result = [(entry['text']) for entry in transcript.translate(learn_code).fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith(learn_code):
                            print(f" ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.  {learn_code})")
                        result = [(entry['text']) for entry in transcript.translate(learn_code).fetch()]
                        return result
                    
                    print("í•œê¸€ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return ""
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
@st.cache_data
def get_best_want_in_time(video_id ,learn_code):
        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
        
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:      
                transcript = transcript_list.find_manually_created_transcript(learn_code)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ë°°ìš¸ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {learn_code} )")
                result = [(entry['text']) for entry in transcript.fetch()]
                return result
            
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(learn_code)
                    print(f"ìë™ ìƒì„±ëœ ë°°ìš¸ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {learn_code} )")
                    result = [(entry['start'] ,entry['text']) for entry in transcript.fetch()]
                    return result
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith('en'):
                            print(f" ë°°ìš¸ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {learn_code} )")
                        result = [(entry['start'] ,entry['text']) for entry in transcript.fetch()]
                        return result
                    
                    print("ì˜ì–´ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            print(f"ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
@st.cache_data
def get_best_learn_code(video_id , learn_code):
        

        # ì„ í˜¸í•˜ëŠ” ì˜ì–´ ìë§‰ ì½”ë“œ ëª©ë¡
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸° (ëª¨ë“  ì˜ì–´ ë³€í˜• ì‹œë„)
            try:
                transcript = transcript_list.find_manually_created_transcript(learn_code)
                print(f"ìˆ˜ë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.{learn_code} ")
                return learn_code
                
            except:
                # 2. ìˆ˜ë™ ìë§‰ì´ ì—†ì„ ê²½ìš°, ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ ì°¾ê¸°
                try:
                    transcript = transcript_list.find_generated_transcript(learn_code)
                    print(f"ìë™ ìƒì„±ëœ ì˜ì–´ ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. {learn_code} ")
                    return learn_code
                    
                except:
                    # 3. ë§ˆì§€ë§‰ìœ¼ë¡œ 'en'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì½”ë“œ í™•ì¸
                    available_transcripts = list(transcript_list)
                    for transcript in available_transcripts:
                        if transcript.language_code.startswith(learn_code):
                            print(f"ìë§‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì–¸ì–´ ì½”ë“œ: {learn_code} ")
                            return learn_code
                    
                    print(" ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None



# video_id ì¶”ì¶œ
user_input= url
video_id = get_video_id(user_input)

title_video = get_video_title(video_id)





def target_translate_isavailable(video_id):
    #transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

    for transcript in transcript_list:
        transcript.is_translatable
        print("í•œêµ­ì–´ ë²ˆì—­ ìë§‰ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤")
        return transcript.is_translatable


def contains_lowercase(eng_script_no_time):
        """ì†Œë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸"""      
        return any(c.islower() for c in eng_script_no_time)
def check_dot(eng_script):
    """ë§ˆì¹¨í‘œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ë§ˆì¹¨í‘œê°€ 5ê°œ ì´í•˜ì¼ ê²½ìš° False ë°˜í™˜)"""
    # ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì¹¨í‘œì˜ ê°œìˆ˜ë¥¼ í™•ì¸
    dot_count = sum(text.count('.') for text in eng_script)
    ja_dot_count = sum(text.count('ã€‚') for text in eng_script)
    # ë§ˆì¹¨í‘œê°€ 5ê°œ ì´í•˜ë¼ë©´ False, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ True ë°˜í™˜
    return dot_count > 5 or ja_dot_count > 5
@st.cache_data
def gemini_check_advanced_word_im_japan(_model, result_eng_transcript, generation_config):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€å®šã®ã‚µã‚¤ã‚ºã«åˆ†å‰²ã—ã€å„éƒ¨åˆ†ã«å¯¾ã—ã¦é«˜åº¦ãªå˜èªã®èª¬æ˜ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¾ã™ã€‚

    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        model: ï¿½ï¿½ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ (APIå‘¼ã³å‡ºã—ç”¨)
        result_eng_transcript: å…¨æ–‡ (æ–‡å­—åˆ—)
        generation_config: ç”Ÿæˆè¨­å®š (genai.types.GenerationConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ)
        chunk_size: å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300)

    æˆ»ã‚Šå€¤:
        adw: é«˜åº¦ãªå˜èªã®èª¬æ˜ãŒå«ã¾ã‚Œã‚‹ãƒªã‚¹ãƒˆ (å„é …ç›®ã¯æ–‡å­—åˆ—)
    """
    # í…ìŠ¤íŠ¸ë¥¼ chunk_size í¬ê¸°ë¡œ ë¶„í• 
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª… ìƒì„±
        try:
            response = model.generate_content(
                f"""{chunk} ãƒ†ã‚­ã‚¹ãƒˆå†…ã«é«˜åº¦ãªå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€æ¬¡ã®å½¢å¼ã§å˜èªã®èª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

                **[æ–‡ä¸­ã®æ™‚é–“] å˜èª : æ„å‘³** - æ–‡è„ˆèª¬æ˜
                - 'æ™‚é–“'ã¯æ–‡ä¸­ã«å˜èªãŒç™»å ´ã™ã‚‹æ™‚åˆ»ã‚’ç¤ºã—ã€'æ„å‘³'ã«ã¯å˜èªã®æ„å‘³ãŒå…¥ã‚Šã¾ã™ã€‚
                - 'æ–‡è„ˆèª¬æ˜'ã¯ãã®å˜èªãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ–‡è„ˆã§ã®æ„å‘³ã‚’è¦ç´„ã—ã¾ã™ã€‚
                ä¾‹ç¤º:
                **[02:12] confirmed : ç¢ºèªã™ã‚‹ - ãƒˆãƒ©ãƒ³ãƒ—ãŒã‚¯ãƒªã‚¹ãƒ†ã‚£ãƒ»ãƒã‚¨ãƒ ã‚’å›½åœŸå®‰å…¨ä¿éšœçœé•·å®˜ã«ä»»å‘½ã™ã‚‹ã¨ã„ã†äº‹å®Ÿã‚’ç¢ºèªã—ãŸã¨ã„ã†æ„å‘³ã§ä½¿ç”¨ã•ã‚Œã¾ã—ãŸã€‚
                ã“ã®å½¢å¼ã«å¾“ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆå†…ã®é«˜åº¦ãªå˜èªã®èª¬æ˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚""",
                generation_config=generation_config
            )
            # ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ response_text ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            response_text.append(response.text)
            response_text.append("\n")

        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì´ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¬¸ì¥ì„ ë¬´ì‹œí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰

    # ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª…ì„ í•œ ë¬¸ìì—´ë¡œ í•©ì¹œ í›„, ê° ì¤„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    advanced_word = "".join(response_text)
    
    return advanced_word
@st.cache_data
def gemini_check_advanced_word_im_china(_model, result_eng_transcript, generation_config):
    """
    å°†æ–‡æœ¬æŒ‰æŒ‡å®šå¤§å°è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸ºæ¯éƒ¨åˆ†ç”Ÿæˆä¸­å›½è¯­çš„é«˜çº§è¯æ±‡è§£é‡Šã€‚

    å‚æ•°:
        model: æ¨¡å‹å¯¹è±¡ (ç”¨äº API è°ƒç”¨)
        result_eng_transcript: å…¨æ–‡ (å­—ç¬¦ä¸²)
        generation_config: ç”Ÿæˆé…ç½® (genai.types.GenerationConfig å¯¹è±¡)
        chunk_size: æ¯ä¸ªåˆ†å—çš„å¤§å° (é»˜è®¤: 300)

    è¿”å›å€¼:
        adw: åŒ…å«é«˜çº§è¯æ±‡è§£é‡Šçš„åˆ—è¡¨ (æ¯ä¸ªé¡¹ç›®ä¸ºå­—ç¬¦ä¸²)
    """
    # å°†æ–‡æœ¬åˆ†å‰²ä¸º chunk_size å¤§å°çš„å—
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆé«˜çº§è¯æ±‡è§£é‡Š
        try:
            response = model.generate_content(
                f"""{chunk} å¦‚æœæ–‡æœ¬ä¸­åŒ…å«é«˜çº§è¯æ±‡ï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›è¯æ±‡è§£é‡Šï¼š

                **[æ–‡æœ¬ä¸­çš„æ—¶é—´] è¯æ±‡ : å«ä¹‰** - ä¸Šä¸‹æ–‡è§£é‡Š
                - 'æ—¶é—´'è¡¨ç¤ºè¯æ±‡å‡ºç°åœ¨æ–‡æœ¬ä¸­çš„æ—¶é—´ï¼Œ'å«ä¹‰'æä¾›è¯æ±‡çš„å«ä¹‰ã€‚
                - 'ä¸Šä¸‹æ–‡è§£é‡Š'æ€»ç»“äº†è¯¥è¯æ±‡åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„å«ä¹‰ã€‚
                ä¾‹å¥:
                **[02:12] confirmed : ç¡®è®¤ - è¿™æ„å‘³ç€ç¡®è®¤äº†ç‰¹æœ—æ™®å°†ä»»å‘½å…‹é‡Œæ–¯è’‚Â·è¯ºåŸƒå§†ä¸ºå›½åœŸå®‰å…¨éƒ¨éƒ¨é•¿çš„äº‹å®ã€‚
                
                è¯·æŒ‰ç…§æ­¤æ ¼å¼ä¸ºæ–‡æœ¬ä¸­çš„é«˜çº§è¯æ±‡ç”Ÿæˆè§£é‡Šã€‚""",
                generation_config=generation_config
            )
            # å°†å“åº”æ–‡æœ¬æ·»åŠ åˆ° response_text åˆ—è¡¨ä¸­
            response_text.append(response.text)
            response_text.append("\n")

        except Exception as e:
            print(f"å‡ºç°é”™è¯¯ï¼Œæ­¤å¥å­å°†è¢«è·³è¿‡: {e}")
            # å¿½ç•¥å‡ºç°é”™è¯¯çš„å¥å­å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå¥å­

    # å°†é«˜çº§è¯æ±‡è§£é‡Šåˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶æŒ‰è¡Œåˆ†å‰²ä¸ºåˆ—è¡¨è¿”å›
    advanced_word = "".join(response_text)
    
    return advanced_word
@st.cache_data     
def gemini_check_advanced_word_im_fran(_model, result_eng_transcript, generation_config):
    """
    Divise le texte en parties de taille spÃ©cifiÃ©e et gÃ©nÃ¨re des explications en franÃ§ais pour les mots avancÃ©s dans chaque partie.

    ParamÃ¨tres:
        model: Objet du modÃ¨le (pour l'appel API)
        result_eng_transcript: Texte complet (chaÃ®ne)
        generation_config: Configuration de gÃ©nÃ©ration (objet genai.types.GenerationConfig)
        chunk_size: Taille de chaque morceau (par dÃ©faut : 300)

    Retour:
        adw: Liste contenant les explications des mots avancÃ©s (chaque Ã©lÃ©ment est une chaÃ®ne de caractÃ¨res)
    """
    # Diviser le texte en morceaux de taille chunk_size
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # Appeler le modÃ¨le pour gÃ©nÃ©rer les explications des mots avancÃ©s
        try:
            response = model.generate_content(
                f"""{chunk} Si le texte contient des mots avancÃ©s, veuillez fournir une explication pour chaque mot selon le format suivant :

                **[Temps dans le texte] Mot : Signification** - Explication contextuelle
                - 'Temps' indique le moment oÃ¹ le mot apparaÃ®t dans le texte, 'Signification' donne le sens du mot.
                - 'Explication contextuelle' rÃ©sume le sens du mot dans le contexte oÃ¹ il est utilisÃ©.
                Exemple :
                **[02:12] confirmed : Confirmer - Cela signifie avoir confirmÃ© le fait que Trump allait nommer Kristi Noem au poste de secrÃ©taire Ã  la SÃ©curitÃ© intÃ©rieure.
                Veuillez gÃ©nÃ©rer des explications pour les mots avancÃ©s dans le texte en suivant ce format.""",
                generation_config=generation_config
            )
            # Ajouter le texte de la rÃ©ponse Ã  la liste response_text
            response_text.append(response.text)
            response_text.append("\n")

        except Exception as e:
            print(f"Erreur rencontrÃ©e, cette phrase sera ignorÃ©e : {e}")
            # Ignorer la phrase avec l'erreur et passer Ã  la suivante

    # Combiner les explications des mots avancÃ©s en une seule chaÃ®ne, puis diviser chaque ligne en liste pour le retour
    advanced_word = "".join(response_text)
    
    return advanced_word
@st.cache_data
def gemini_check_advanced_word_im_espanol(_model, result_eng_transcript, generation_config):
    """
    Divide el texto en partes de tamaÃ±o especificado y genera explicaciones en espaÃ±ol para las palabras avanzadas en cada parte.

    ParÃ¡metros:
        model: Objeto del modelo (para la llamada a la API)
        result_eng_transcript: Texto completo (cadena)
        generation_config: ConfiguraciÃ³n de generaciÃ³n (objeto genai.types.GenerationConfig)
        chunk_size: TamaÃ±o de cada fragmento (por defecto: 300)

    Retorno:
        adw: Lista que contiene explicaciones de palabras avanzadas (cada elemento es una cadena de caracteres)
    """
    # Dividir el texto en fragmentos de tamaÃ±o chunk_size
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # Llamar al modelo para generar explicaciones de palabras avanzadas
        try:
            response = model.generate_content(
                f"""{chunk} Si el texto contiene palabras avanzadas, proporcione una explicaciÃ³n para cada palabra segÃºn el siguiente formato:

                **[Tiempo en el texto] Palabra : Significado** - ExplicaciÃ³n contextual
                - 'Tiempo' indica el momento en el que aparece la palabra en el texto, 'Significado' proporciona el significado de la palabra.
                - 'ExplicaciÃ³n contextual' resume el significado de la palabra en el contexto en el que se utiliza.
                Ejemplo:
                **[02:12] confirmed : Confirmar - Esto significa haber confirmado que Trump nombrarÃ¡ a Kristi Noem como secretaria de Seguridad Nacional.
                Genere explicaciones para las palabras avanzadas en el texto siguiendo este formato.""",
                generation_config=generation_config
            )
            # Agregar el texto de la respuesta a la lista response_text
            response_text.append(response.text)
            response_text.append("\n")

        except Exception as e:
            print(f"Se encontrÃ³ un error, se omitirÃ¡ esta frase: {e}")
            # Ignorar la frase con el error y pasar a la siguiente

    # Combinar las explicaciones de palabras avanzadas en una sola cadena, luego dividir cada lÃ­nea en lista para el retorno
    advanced_word = "".join(response_text)
    
    return advanced_word
@st.cache_data
def gemini_check_advanced_word(_model, result_eng_transcript, generation_config):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• í•˜ê³  ê° ì¡°ê°ì— ëŒ€í•´ ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
        model: ëª¨ë¸ ê°ì²´ (API í˜¸ì¶œìš©)
        result_eng_transcript: ì „ì²´ í…ìŠ¤íŠ¸ (string)
        generation_config: ìƒì„± ì„¤ì • (genai.types.GenerationConfig ê°ì²´)
        chunk_size: ê° ì²­í¬ì˜ í¬ê¸° (ê¸°ë³¸ê°’: 300)
    
    Returns:
        adw: ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª…ì´ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ ë¬¸ìì—´)
    """
    # í…ìŠ¤íŠ¸ë¥¼ chunk_size í¬ê¸°ë¡œ ë¶„í• 
    chunk_size = 300 
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª… ìƒì„±
        try:
            response = model.generate_content(
                f"""{chunk} í…ìŠ¤íŠ¸ ë‚´ì— ê³ ê¸‰ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹¨ì–´ ì„¤ëª…ì„ ì œê³µí•´ ì£¼ì„¸ìš”:

                **[ë¬¸ì¥ ë‚´ ì‹œê°„] ë‹¨ì–´ : ëœ»** - ë¬¸ë§¥ ì„¤ëª…
                - 'ì‹œê°„'ì€ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ê°€ í¬í•¨ëœ ì‹œê°„ì„ ë‚˜íƒ€ë‚´ë©°, 'ëœ»'ì—ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
                - 'ë¬¸ë§¥ ì„¤ëª…'ì€ í•´ë‹¹ ë‹¨ì–´ê°€ ì‚¬ìš©ëœ ë§¥ë½ì—ì„œ ì–´ë–¤ ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ëŠ”ì§€ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

                ì˜ˆì‹œ:
                **[02:12] confirmed : í™•ì¸í•˜ë‹¤** - íŠ¸ëŸ¼í”„ê°€ í¬ë¦¬ìŠ¤í‹° ë…¸ì„ì„ êµ­í† ì•ˆë³´ë¶€ ì¥ê´€ìœ¼ë¡œ ì„ëª…í•  ê²ƒì´ë¼ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í–ˆë‹¤ëŠ” ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤

                ì´ í˜•ì‹ì„ ì°¸ê³ í•˜ì—¬ í…ìŠ¤íŠ¸ ë‚´ ê³ ê¸‰ ë‹¨ì–´ë“¤ì˜ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.""",
                generation_config=generation_config
            )
            # ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ response_text ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì´ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¬¸ì¥ì„ ë¬´ì‹œí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
            

    # ê³ ê¸‰ ë‹¨ì–´ ì„¤ëª…ì„ í•œ ë¬¸ìì—´ë¡œ í•©ì¹œ í›„, ê° ì¤„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    advanced_word = "".join(response_text)
    
    return advanced_word 
@st.cache_data
def gemini_translate_text(_model, result_eng_transcript, generation_config):
                                    
    """
    í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• í•˜ê³  ê° ì¡°ê°ì— ë²ˆì—­ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
        model: ëª¨ë¸ ê°ì²´ (API í˜¸ì¶œìš©)
        result_eng_transcript: ì „ì²´ í…ìŠ¤íŠ¸ (string)
        generation_config: ìƒì„± ì„¤ì • (genai.types.GenerationConfig ê°ì²´)
        chunk_size: ê° ì²­í¬ì˜ í¬ê¸° (ê¸°ë³¸ê°’: 300)
    
    Returns:
        translated_text: ë²ˆì—­ëœ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë¬¸ìì—´
    """
    # í…ìŠ¤íŠ¸ë¥¼ chunk_size í¬ê¸°ë¡œ ë¶„í• 
    chunk_size= 200
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ë²ˆì—­ ìƒì„±
        try:
            response = model.generate_content(
                f"""
                        ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•´ì£¼ì„¸ìš”.
                        
                       

                        ì›ë³¸ í…ìŠ¤íŠ¸: " {chunk}"

                        ë²ˆì—­ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
                        1. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
                        2. ì „ë¬¸ ìš©ì–´ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë¶„ì•¼ì—ì„œ í†µìš©ë˜ëŠ” ì •í™•í•œ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
                        3. ë¬¸í™”ì  ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
                        4. ì¡´ëŒ“ë§ì´ë‚˜ ê²©ì‹ì²´ì˜ ìˆ˜ì¤€ì„ ì›ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
                        
                        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
                        
                        [ë¬¸ì¥ ë‚´ ì‹œê°„] ë²ˆì—­
                        ì˜ˆì‹œ
                        [00:03] ì¼ì°ì´ ì €ëŠ” ì¿ ë¥´íŠ¸  í†µ ì „ ë¯¸êµ­ APEC ëŒ€ì‚¬ì´ì ì•„ì‹œì•„ ê·¸ë£¹ì˜ ë§¤ë‹ˆì§• íŒŒíŠ¸ë„ˆì™€ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ´ìŠµë‹ˆë‹¤.
    

                        ì¶”ê°€ ì§€ì¹¨:
                        - ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
                        - ì„¤ëª…ì´ë‚˜ ì£¼ì„ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
                        
                        """,
                generation_config=generation_config
            )
            # ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ response_text ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì´ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¬¸ì¥ì„ ë¬´ì‹œí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰

    # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ë°˜í™˜
    translated_text = "".join(response_text)
    
    return translated_text 
@st.cache_data
def gemini_translate_text_im_japan(_model, result_eng_transcript, generation_config):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€å®šã®ã‚µã‚¤ã‚ºã«åˆ†å‰²ã—ã€å„éƒ¨åˆ†ã«ã¤ã„ã¦ç¿»è¨³ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        model: ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ (APIå‘¼ã³å‡ºã—ç”¨)
        result_eng_transcript: å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ (æ–‡å­—åˆ—)
        generation_config: ç”Ÿæˆè¨­å®š (genai.types.GenerationConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ)
        chunk_size: å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300)
    
    æˆ»ã‚Šå€¤:
        translated_text: ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€æ–‡å­—åˆ—
    """
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’chunk_sizeã®å¤§ãã•ã§åˆ†å‰²
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ç¿»è¨³ã‚’ç”Ÿæˆ
        try:
            response = model.generate_content(
                f"""
                ã‚ãªãŸã¯å°‚é–€ã®ç¿»è¨³è€…ã§ã™ã€‚æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

                åŸæ–‡: "{chunk}"

                ç¿»è¨³ã®éš›ã€ä»¥ä¸‹ã®ç‚¹ã«å¾“ã£ã¦ãã ã•ã„:
                1. åŸæ–‡ã®æ„å‘³ã‚’æ­£ç¢ºã«ä¼ãˆã¤ã¤ã€è‡ªç„¶ãªè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
                2. å°‚é–€ç”¨èªãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã®åˆ†é‡ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹æ­£ç¢ºãªç”¨èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
                3. æ–‡åŒ–çš„ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ã€é©åˆ‡ãªè¡¨ç¾ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
                4. æ•¬èªã‚„ä¸å¯§èªã®ãƒ¬ãƒ™ãƒ«ã‚’åŸæ–‡ã¨åŒã˜ã«ã—ã¦ãã ã•ã„ã€‚

                ã€Œæ¬¡ã®å½¢å¼ã§æ›¸ã„ã¦ãã ã•ã„ã€

                [æ–‡ä¸­ã®æ™‚é–“] ç¿»è¨³
                ä¾‹ãˆ
                [00:03] ä»¥å‰ã€ç§ã¯å…ƒã‚¢ãƒ¡ãƒªã‚«APECå¤§ä½¿ã§ã‚ã‚Šã€ã‚¢ã‚¸ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒãƒ¼ã‚¸ãƒ³ã‚°ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã§ã‚ã‚‹ã‚¯ãƒ«ãƒˆãƒ»ãƒˆãƒ³ã‚°æ°ã¨ãŠè©±ã—ã—ã¾ã—ãŸã€‚

                è¿½åŠ æŒ‡ç¤º:
                - ç¿»è¨³æ–‡ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                - èª¬æ˜ã‚„æ³¨é‡ˆã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„ã€‚
                """,
                generation_config=generation_config
            )
            # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’response_textãƒªã‚¹ãƒˆã«è¿½åŠ 
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ã“ã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸæ–‡ã‚’ç„¡è¦–ã—ã¦æ¬¡ã«é€²ã‚€

    # ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’1ã¤ã®æ–‡å­—åˆ—ã«çµåˆã—ã¦è¿”ã™
    translated_text = "".join(response_text)
    
    return translated_text
@st.cache_data
def gemini_translate_text_im_china(_model, result_eng_transcript, generation_config):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€å®šã®ã‚µã‚¤ã‚ºã«åˆ†å‰²ã—ã€å„éƒ¨åˆ†ã«ã¤ã„ã¦ç¿»è¨³ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        model: ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ (APIå‘¼ã³å‡ºã—ç”¨)
        result_eng_transcript: å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ (æ–‡å­—åˆ—)
        generation_config: ç”Ÿæˆè¨­å®š (genai.types.GenerationConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ)
        chunk_size: å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300)
    
    æˆ»ã‚Šå€¤:
        translated_text: ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€æ–‡å­—åˆ—
    """
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’chunk_sizeã®å¤§ãã•ã§åˆ†å‰²
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ç¿»è¨³ã‚’ç”Ÿæˆ
        try:
            response = model.generate_content(
                f"""
                ã‚ãªãŸã¯å°‚é–€ã®ç¿»è¨³è€…ã§ã™ã€‚æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

                åŸæ–‡: "{chunk}"

                ç¿»è¨³ã®éš›ã€ä»¥ä¸‹ã®ç‚¹ã«å¾“ã£ã¦ãã ã•ã„:
                1. åŸæ–‡ã®æ„å‘³ã‚’æ­£ç¢ºã«ä¼ãˆã¤ã¤ã€è‡ªç„¶ãªè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
                2. å°‚é–€ç”¨èªãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã®åˆ†é‡ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹æ­£ç¢ºãªç”¨èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
                3. æ–‡åŒ–çš„ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ã€é©åˆ‡ãªè¡¨ç¾ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
                4. æ•¬èªã‚„ä¸å¯§èªã®ãƒ¬ãƒ™ãƒ«ã‚’åŸæ–‡ã¨åŒã˜ã«ã—ã¦ãã ã•ã„ã€‚
                
                ã€Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ä¹¦å†™ã€
                [å¥å­ä¸­çš„æ—¶é—´] ç¿»è¯‘

                ä¾‹å­
                [00:03] æ­¤å‰ï¼Œæˆ‘ä¸å‰ç¾å›½APECå¤§ä½¿ã€äºšæ´²é›†å›¢çš„ç®¡ç†åˆä¼™äººåº“å°”ç‰¹Â·é€šå…ˆç”Ÿè¿›è¡Œäº†äº¤è°ˆã€‚
                
                è¿½åŠ æŒ‡ç¤º:
                - ç¿»è¨³æ–‡ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                - èª¬æ˜ã‚„æ³¨é‡ˆã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„ã€‚
                """,
                generation_config=generation_config
            )
            # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’response_textãƒªã‚¹ãƒˆã«è¿½åŠ 
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ã“ã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸæ–‡ã‚’ç„¡è¦–ã—ã¦æ¬¡ã«é€²ã‚€

    # ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’1ã¤ã®æ–‡å­—åˆ—ã«çµåˆã—ã¦è¿”ã™
    translated_text = "".join(response_text)
    
    return translated_text
@st.cache_data
def gemini_translate_text_im_espanol(_model, result_eng_transcript, generation_config):

    """
    Divide el texto en partes de un tamaÃ±o especÃ­fico y genera la traducciÃ³n para cada fragmento.
    
    ParÃ¡metros:
        model: objeto del modelo (para la llamada API)
        result_eng_transcript: texto completo (cadena de texto)
        generation_config: configuraciÃ³n de generaciÃ³n (objeto genai.types.GenerationConfig)
        chunk_size: tamaÃ±o de cada fragmento (predeterminado: 300)
    
    Retorna:
        translated_text: cadena de texto que contiene el texto traducido
    """
    # Divide el texto en fragmentos de tamaÃ±o chunk_size
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # Llama al modelo para generar la traducciÃ³n
        try:
            response = model.generate_content(
                f"""
                Eres un traductor profesional. Traduce el siguiente texto.

                Texto original: "{chunk}"

                Al traducir, sigue estos puntos:
                1. Transmite el significado del texto original con precisiÃ³n y utiliza una expresiÃ³n natural.
                2. Si contiene tÃ©rminos tÃ©cnicos, utiliza el tÃ©rmino correcto que se usa comÃºnmente en el campo correspondiente.
                3. Considera el contexto cultural y conviÃ©rtelo a una expresiÃ³n adecuada.
                4. MantÃ©n el nivel de cortesÃ­a o formalidad igual al del texto original.

                Por favor, escrÃ­balo en el siguiente formato.

                [Tiempo en la oraciÃ³n] TraducciÃ³n

                ejemplo
                [00:03] Anteriormente, hablÃ© con Kurt Tong, ex embajador de EE. UU. en APEC y socio gerente del Grupo Asia.
                
                Instrucciones adicionales:
                - Solo muestra el texto traducido.
                - No aÃ±adas explicaciones ni notas.
                """,
                generation_config=generation_config
            )
            # Agrega el texto de respuesta a la lista response_text
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"Se produjo un error, esta oraciÃ³n se omitirÃ¡: {e}")
            # Ignora la oraciÃ³n en la que se produjo un error y continÃºa

    # Une el texto traducido en una sola cadena y lo retorna
    translated_text = "".join(response_text)
    
    return translated_text
@st.cache_data
def gemini_translate_text_im_fran(_model, result_eng_transcript, generation_config): 
    """
    Divise le texte en parties de taille spÃ©cifique et gÃ©nÃ¨re la traduction pour chaque fragment.
    
    ParamÃ¨tres :
        model : objet du modÃ¨le (pour l'appel API)
        result_eng_transcript : texte complet (chaÃ®ne de caractÃ¨res)
        generation_config : configuration de gÃ©nÃ©ration (objet genai.types.GenerationConfig)
        chunk_size : taille de chaque fragment (par dÃ©faut : 300)
    
    Retourne :
        translated_text : chaÃ®ne de caractÃ¨res contenant le texte traduit
    """
    # Divise le texte en fragments de taille chunk_size
    chunk_size = 300
    chunks_script = [result_eng_transcript[i:i + chunk_size] for i in range(0, len(result_eng_transcript), chunk_size)]
    response_text = []

    for chunk in chunks_script:
        # Appelle le modÃ¨le pour gÃ©nÃ©rer la traduction
        try:
            response = model.generate_content(
                f"""
                Vous Ãªtes un traducteur professionnel. Traduisez le texte suivant.

                Texte original : "{chunk}"

                Lors de la traduction, veuillez respecter les points suivants :
                1. Transmettez prÃ©cisÃ©ment le sens du texte original tout en utilisant une expression naturelle.
                2. Si des termes techniques sont inclus, utilisez le terme correct couramment employÃ© dans le domaine concernÃ©.
                3. Prenez en compte le contexte culturel et adaptez l'expression de maniÃ¨re appropriÃ©e.
                4. Maintenez le mÃªme niveau de politesse ou de formalitÃ© que dans le texte original.

                Veuillez l'Ã©crire dans le format suivant
                [Temps dans la phrase] Traduction

                exemple
                [00:03] Auparavant, j'ai parlÃ© avec Kurt Tong, ancien ambassadeur des Ã‰tats-Unis auprÃ¨s de l'APEC et partenaire directeur du Groupe Asie.
                
                Instructions supplÃ©mentaires :
                - Affichez uniquement le texte traduit.
                - N'ajoutez ni explications ni notes.
                """,
                generation_config=generation_config
            )
            # Ajoute le texte de rÃ©ponse Ã  la liste response_text
            response_text.append(response.text)
            response_text.append("\n")
        
        except Exception as e:
            print(f"Une erreur est survenue, cette phrase sera ignorÃ©e : {e}")
            # Ignore la phrase ayant causÃ© une erreur et continue

    # ConcatÃ¨ne le texte traduit en une seule chaÃ®ne et la retourne
    translated_text = "".join(response_text)
    
    return translated_text


import streamlit as st

def create_settings_sidebar():
    with st.sidebar:
        st.title("ğŸ› ï¸ SETTING")
        
        # Gemini API ì„¤ì • ì„¹ì…˜
        st.header("API Setting")
        api_key = st.text_input(
            "Gemini API Key",
            type="password",
            help="Please enter the Gemini API key issued from Google Cloud Console."
        )
         # API í‚¤ ë°œê¸‰ ë§í¬ ë²„íŠ¼
        st.markdown("""
            <a href="https://aistudio.google.com/apikey?hl=ko&_gl=1*1841s3x*_ga*MTk1Nzc2OTYwMi4xNzMwOTQyNDU0*_ga_P1DBVKWT6V*MTczMTcyOTY1MC4xMC4wLjE3MzE3Mjk2NTAuNjAuMC4xODI4NzE1NDg." 
               target="_blank"
               style="text-decoration: none;">
                <div style="background-color: #619BF7; 
                            padding: 10px 20px; 
                            border-radius: 25px; 
                            text-align: center; 
                            margin: 10px 0;
                            display: inline-block;
                            width: 100%;
                            box-sizing: border-box;
                            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                            box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24);">
                    <div style="display: flex; 
                                align-items: center; 
                                justify-content: center; 
                                gap: 8px;
                                color: #1a1a1a;
                                font-size: 14px;
                                font-weight: 500;
                                letter-spacing: 0.25px;">
                        <span style="color: #1a1a1a;">ğŸ”‘</span>
                        <span style="color: #1a1a1a;">Get your Api key.</span>
                    </div>
                </div>
            </a>
            """, unsafe_allow_html=True)
        
        st.header("Font size Setting")
        
        # ëª¨êµ­ì–´ì™€ í•™ìŠµ ì–¸ì–´ì˜ í°íŠ¸ ì„¤ì •
        native_font = st.text_input(
            "Native Language font",
            value="Noto Sans",
            help="default setting : Noto Sans"
        )
        
        want_font = st.text_input(
            "Want Learn Language font",
            value="Cambria",
            help="default setting : Cambria"
        )
        
        # í°íŠ¸ í¬ê¸° ì„¤ì •
        font_size = st.slider(
            "Font Size (px)",
            min_value=8,
            max_value=20,
            value=11,
            step=1
        )
        
        return {
            "api_key": api_key,
            "native_font": native_font,
            "want_font": want_font,
            "font_size": font_size,
        }

settings = create_settings_sidebar()

if not settings["api_key"]:
        st.warning("Please enter the Gemini API key in the sidebar.")
        st.stop()
st.write(f"- Slected native font: {settings['native_font']}")
st.write(f"- Slected learning font: {settings['want_font']}")
st.write(f"- Font Size: {settings['font_size']}px")

native_font = settings['native_font']
want_font = settings['want_font']
font_size = settings['font_size']
api_key = settings['api_key']

if video_id is None :
    display_chat_message("assistant", "Please check the URL address again.")
    
else:
    try:
        # transcript_list ì´ˆê¸°í™”
        
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        
        # ì±„íŒ… ì²˜ë¦¬
        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            display_chat_message("user", user_input)

            # AI ì‘ë‹µ (ì˜ˆì‹œ)
            ai_response = f"We are processing the message you entered: '**{user_input}**'."

            try:
                # ìë§‰ ì–¸ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                    display_chat_message("assistant", ai_response)
    
                #if want_language == "English":# ì˜ì–´ê°€ ë°°ìš°ê³  ì‹¶ì€ ì‚¬ëŒì´ ë§ì„í…Œë‹ˆ í˜ì„ ì¼ë‹¤.
                    

                    if want_language == "English":

                        want_lang_no_time = get_best_english_transcript_no_time(video_id)
                        want_code_check = get_best_english_encode(video_id)
                        want_lang_in_time = get_best_english_transcript(video_id)
                       
                        
                    else:
                        want_lang_no_time = get_best_want_no_time(video_id, learn_code)
                        want_code_check  = get_best_learn_code(video_id , learn_code)
                        want_lang_in_time= get_best_want_in_time(video_id,learn_code)
                       
                    
                    if want_code_check == None:
                        display_chat_message("assistant" , "There are no subtitles available. Please choose a different video.")
                        

                    else:                                     
                           
                            UporLow = contains_lowercase(want_lang_no_time)
                            dot_Check = check_dot(want_lang_no_time)
                        
                            if dot_Check == False:
                                if want_language  == "English":                                      
                                    display_chat_message("assistant", "There are no delimiters detected. This service analyzes sentences based on delimiters such as (.) and (?). Weâ€™ll try a different way to separate the sentences, but this method might not be as precise.")
                                else:
                                    #display_chat_message("assistant", "There are no delimiters. This service analyzes sentences using delimiters such as (.) and (?). Please select a different video.")
                                    st.warning("There are no delimiters. This service analyzes sentences using delimiters such as (.) and (?). Please select a different video.")
                                    st.stop()
                        
                            
                            
                            import re 

                            new_script = ""

                            for start, read_script in want_lang_in_time:
                                    minutes = int(start // 60)  # ë¶„ ê³„ì‚° (ì†Œìˆ˜ì  ì—†ìŒ)
                                    seconds = int(start % 60)  # ì´ˆ ê³„ì‚° (ì†Œìˆ˜ì  ì—†ìŒ)

                                    if dot_Check == False and want_language  == "English": # ì˜ì–´ ì¼ë•Œë§Œ ë¬¸ì¥ êµ¬ë¶„ 
                                        #display_chat_message("assistant", dot_Check)
                                            # ë¬¸ì¥ êµ¬ë¶„ì´ í•„ìš”í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
                                        keywords = ["I", "And", "But", "Now", "What", "How", "Have", "Did", "No", "In", 
                                        "So", "Then", "Or", "Why", "Yes", "If", "When", "Because", 
                                        "Well", "Oh", "Ah", "Okay", "Alright", 
                                        "Therefore", "However", "Moreover", "Though", "Although"]
                                        for word in keywords:
                                            read_script  = read_script.replace(word , f".{word}")
                                            
                                
                                # ì‹œê°„ í˜•ì‹ ì„¤ì • (ë¶„.ì´ˆ í˜•íƒœ)
                                    time_format = f"[{minutes:02d}:{seconds:02d}]"
                                # . ê¸°ë°˜ì´ë‹¤ ë³´ë‹ˆ ë¬¸ì œê°€ ìˆì„ ë§Œí•œ ê²ƒë“¤ì„ ìˆ˜ì •    
                                    read_script = read_script.replace('U.S.', 'US')
                                    read_script = read_script.replace('U.S', 'US')
                                    read_script = read_script.replace('S.E.C.' , 'SEC')
                                    read_script = read_script.replace('Mr.', 'Mr ')
                                    read_script = read_script.replace('Mrs.', 'Mrs ')

                                    read_script = read_script.replace('Ph.D.', 'ph,D ')
                                    read_script = read_script.replace('Prof.', 'prof ')
                                    read_script = read_script.replace('Dr.', 'Dr ')

                                    read_script = read_script.replace('No.', 'Number ')
                                    
                                    read_script = read_script.replace('a.m.', 'am')
                                    read_script = read_script.replace('p.m.', 'pm')
                                    
                                    read_script = re.sub(r'(\d)\.(\d)', r'\1 point \2', read_script)


                                    read_script = read_script.replace('..' , ".")
                                    read_script = read_script.replace('..' , ".")
                                    read_script = read_script.replace('..' , ".") 
                                    #read_script = read_script.replace('. >>' , ".")

                                    read_script = read_script.replace('\n', ' ')
                                    read_script = read_script.replace('.', '. \n')
                                    read_script = read_script.replace('?' , '? \n')
                                    read_script = read_script.replace('ã€‚' , 'ã€‚ \n')
                                   
                                    if UporLow  == False:
                                            read_script = read_script[0].upper() + read_script[1:].lower()

                                    new_script += ' '
                                    new_script += time_format
                                    new_script += read_script

                                
                            result_want_transcript =  ["\n\n"]

                            #to_timestamps_list = []

                            # íƒ€ì„ìŠ¤íƒ¬í”„ ë§¨ì• ë¹¼ê³  ì œê±° í•¨ìˆ˜
                            def clean_transcript_texts(transcript_texts):
                                        cleaned_texts = ""
                                        for text in transcript_texts:
                                            # ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì œê±°
                                            # 1) ëª¨ë“  íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì°¾ìŒ
                                            timestamps = re.findall(r'\[\d{2}:\d{2}\]', text)
                                            
                                            if timestamps:
                                                # 2) ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ë‚¨ê¸°ê³   ë¦¬ìŠ¤íŠ¸ì— to_time  ì— ë„£ì–´ì£¼ì—ˆìŒ ë‚˜ì¤‘ì— ì•ì— ë¶™ì¼ê±°
                                                first_timestamp = timestamps[0]
                                                #to_timestamps_list.append(first_timestamp)
                                                cleaned_text = text.replace(first_timestamp, '',1)
                                                cleaned_text = re.sub(r'\[\d{2}:\d{2}\]','', cleaned_text)  # ë‚˜ë¨¸ì§€ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
                                                cleaned_texts += first_timestamp +" "+ cleaned_text.strip() +" "
                                            
                                            else:
                                                # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ëŠ” ê²½ìš°
                                                cleaned_texts += text.strip() +" "

                                        return cleaned_texts.strip()  # ìµœì¢… ë¬¸ìì—´ ë°˜í™˜

                            result_want_script = new_script.splitlines()

                            for line in  result_want_script:
                                    result_want_transcript.append("\n")
                                    result_want_transcript.append(clean_transcript_texts([line]))
                                    result_want_transcript.append("\n")    

                            result_only_want_for_word = ["\n\n\n"]
                            
                            for line in  result_want_script:
                                    
                                    result_only_want_for_word.append(clean_transcript_texts([line]))
                                    result_only_want_for_word.append("\n")
                                               
                            
                            word_file = create_word_file_shadow_script(result_only_want_for_word,title_video,learn_code,want_font,native_font,font_size)
                            

                            result_target_script =get_best_to_translate_target(video_id , native_code)
                            new_target_script = ""

                            for read_script_target_line in result_target_script:
                                    
                                    read_script_target_line = read_script_target_line.replace('U.S.', 'US')
                                    read_script_target_line = read_script_target_line.replace('U.S', 'US')
                                    read_script_target_line = read_script_target_line.replace('S.E.C.' , 'SEC')
                                    read_script_target_line = read_script_target_line.replace('Mr.', 'Mr ')
                                    read_script_target_line = read_script_target_line.replace('Mrs.', 'Mrs ')

                                    read_script_target_line = read_script_target_line.replace('Ph.D.', 'ph,D ')
                                    read_script_target_line = read_script_target_line.replace('Prof.', 'prof ')
                                    read_script_target_line = read_script_target_line.replace('Dr.', 'Dr ')

                                    read_script_target_line = read_script_target_line.replace('No.', 'Number')
                                    read_script_target_line = read_script_target_line.replace('a.m.', 'am')
                                    read_script_target_line= read_script_target_line.replace('p.m.', 'pm')

                                    read_script_target_line = read_script_target_line.replace('\n', ' ')
                                    read_script_target_line = read_script_target_line.replace('.', '. \n')
                                    read_script_target_line = read_script_target_line.replace('ã€‚', 'ã€‚ \n')
                                    read_script_target_line = read_script_target_line.replace('?' , '? \n')
                                    
                                    
                                    new_target_script +=' '
                                    new_target_script += read_script_target_line
                            
                            
                            kor_script_line = new_target_script.splitlines()

                            #display_chat_message("assistant", result_target_script)
                            
                            import google.generativeai as genai
                                                            
                            try:
                                genai.configure(api_key=api_key)
                                model = genai.GenerativeModel("gemini-1.5-flash")
                                generation_config = genai.types.GenerationConfig(
                                    candidate_count=1,
                                    stop_sequences=["x"],
                                    temperature=0,
                                )
                            except Exception as e:
                                print(f"An error occurred: {e}")
                                # Add further actions like retrying or prompting for a valid API key.
                                st.warning("Please check the Gemini API again.")
                                st.stop()

                            #ì–¸ì–´ ë¶„ì„ì€ ì´ë ‡ê²Œ ê°€ì 
                            if native_code == "ja":
                                    advanced_word = gemini_check_advanced_word_im_japan(model, result_want_transcript, generation_config)
                            if native_code == "ko":
                                advanced_word = gemini_check_advanced_word(model, result_want_transcript, generation_config)

                            if native_code == "es":
                                    advanced_word = gemini_check_advanced_word_im_espanol(model, result_want_transcript, generation_config)
                            if native_code == "zh-Hans":
                                    advanced_word = gemini_check_advanced_word_im_china(model, result_want_transcript, generation_config)

                            if native_code == "fr":
                                    advanced_word =gemini_check_advanced_word_im_fran(model, result_want_transcript, generation_config)
                            
                            adw_script = advanced_word.splitlines()    
                            word_file_adw = create_word_file_shadow_script(adw_script ,title_video,learn_code,want_font,native_font,font_size)

                            display_chat_message("assistant","I'm working hard on the analysis, but it might take some time. Please wait a moment!") 
                            
                            #display_chat_message("assistant", advanced_word)
                            if native_code == "ja":
                                gemini_transcript= gemini_translate_text_im_japan(model, result_want_transcript, generation_config)
                            if native_code == "ko":
                                gemini_transcript= gemini_translate_text(model, result_want_transcript, generation_config)

                            if native_code == "es":
                                    gemini_transcript= gemini_translate_text_im_espanol(model, result_want_transcript, generation_config)
                            if native_code == "zh":
                                    gemini_transcript= gemini_translate_text_im_china(model, result_want_transcript, generation_config)

                            if native_code == "fr":
                                    gemini_transcript= gemini_check_advanced_word_im_fran(model, result_want_transcript, generation_config)

                            

                            #ì›ë˜ëŠ” ë¬¸ì œê°€ ìˆëŠ” ê°œë³„ì„ ë²ˆì—­í•´ì£¼ë ¤í–ˆìœ¼ë‚˜ ê°€ë” ì˜¤ë¥˜ê°€ ë°œìƒ ì‚¬ìš©ë¶ˆê°€
                            def translate_with_gemini(model, text, source_lang, target_lang):
                                        """
                                        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
                                        
                                        ë§¤ê°œë³€ìˆ˜:
                                        model: Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
                                        text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
                                        source_lang (str): ì›ë³¸ ì–¸ì–´ (ì˜ˆ: 'í•œêµ­ì–´', 'ì˜ì–´', 'ì¼ë³¸ì–´')
                                        target_lang (str): ëª©í‘œ ì–¸ì–´ (ì˜ˆ: 'ì˜ì–´', 'í•œêµ­ì–´', 'ì¼ë³¸ì–´')
                                        generation_config: ëª¨ë¸ì˜ ìƒì„± ì„¤ì •
                                        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
                                        
                                        ë°˜í™˜:
                                        str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
                                        """
                                        attempt = 0
                                        while attempt < 3:
                                            try:
                                                response = model.generate_content(
                                                    f"""
                                                    ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {source_lang}ì—ì„œ {target_lang}ìœ¼ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

                                                    ì›ë³¸ í…ìŠ¤íŠ¸: "{text}"

                                                    ë²ˆì—­ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
                                                    1. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
                                                    2. ì¡´ëŒ“ë§ì´ë‚˜ ê²©ì‹ì²´ì˜ ìˆ˜ì¤€ì„ ì›ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
                                                    3. ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ë©° ì™„ë²½í•˜ê²Œ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ëŠ” ë²ˆì—­ê°€ì…ë‹ˆë‹¤. 
                                                    
                                                    ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
                                                    """,
                                                    generation_config=generation_config
                                                )
                                                
                                                # ì‘ë‹µì´ ìœ íš¨í•œì§€ í™•ì¸
                                                if hasattr(response, 'text') and response.text:
                                                    return response.text
                                                else:
                                                    raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µì…ë‹ˆë‹¤.")
                                            
                                            except Exception as e:
                                                print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ (ì‹œë„ {attempt + 1}/3): {e}")
                                                attempt += 1

                                        # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ None ë°˜í™˜
                                        print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                                        return None

                            from sentence_transformers import SentenceTransformer  # í…ìŠ¤íŠ¸ ë°±í„° ë³€í™˜
                            from sklearn.metrics.pairwise import cosine_similarity # ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
                            import numpy as np

                            # íŒŒì¼ ì½ê¸° ë¦¬ìŠ¤íŠ¸í™” í•˜ì˜€ìŠµë‹ˆë‹¤
                            
                            english_lines = result_want_transcript

                            gemini_lines = gemini_transcript.splitlines()  # splitlines()ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                            korean_lines = kor_script_line
                            #display_chat_message("assistant",gemini_lines)
                            # ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš©)
                            #model_similarity = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')#paraphrase-xlm-r-multilingual-v1
                            # íŒŒì¼ ì½ê¸° ë¦¬ìŠ¤íŠ¸í™” í•˜ì˜€ìŠµë‹ˆë‹¤
                            

                            # ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš©)
                            #model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')#paraphrase-xlm-r-multilingual-v1
                            model_simul = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')
                            # ì˜ì–´ì™€ í•œê¸€ ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„° ìƒì„± # ì„ë² ë”© ìƒì„±
                            english_embeddings = model_simul .encode(english_lines)
                            korean_embeddings = model_simul .encode(korean_lines)

                            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
                            similarity_matrix = cosine_similarity(english_embeddings, korean_embeddings) 

                            # ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë¬¸ì¥ë¼ë¦¬ ë§¤ì¹­
                            merged_lines = ["\n\n\n"]
                            used_korean_indices = set() # ì‚¬ìš©í•œ í•œêµ­ì–´ëŠ” ì§€ìš°ê¸° ìœ„í•´ ì§‘í•© ì‚¬ìš©

                            for eng_idx, eng_sentence in enumerate(english_lines):
                                # ê° ì˜ì–´ ë¬¸ì¥ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ í•œê¸€ ë¬¸ì¥ì„ ì°¾ìŒ
                                if not eng_sentence.strip():
                                    continue

                                time_judge = re.search(r"\[(\d{2}:\d{2})\]", eng_sentence)
                                
                                if time_judge:  # time_judgeê°€ Noneì´ ì•„ë‹ ë•Œ
                                    time_str = time_judge.group(0) 

                                
                                    for j in range(len(gemini_lines)):
                                            if time_str in gemini_lines[j]:  # time_strì´ adw[j]ì— ìˆëŠ”ì§€ í™•ì¸
                                                kor_sentence = gemini_lines[j]
                                            else:
                                                best_kor_idx = np.argmax(similarity_matrix[eng_idx])
                                                best_kor_similarity = similarity_matrix[eng_idx, best_kor_idx]             

                                                if best_kor_idx not in used_korean_indices:
                                                                    
                                                        kor_sentence = korean_lines[best_kor_idx]

                                                used_korean_indices.add(best_kor_idx)
                                    
                                                kor_sentence = re.sub(r'\[\d{2}:\d{2}\]','', kor_sentence)
                               
                                else:
                                    best_kor_idx = np.argmax(similarity_matrix[eng_idx])
                                    best_kor_similarity = similarity_matrix[eng_idx, best_kor_idx]             

                                    if best_kor_idx not in used_korean_indices:
                                                        
                                            kor_sentence = korean_lines[best_kor_idx]

                                    used_korean_indices.add(best_kor_idx)
                        
                                    kor_sentence = re.sub(r'\[\d{2}:\d{2}\]','', kor_sentence)
                    
                                    
                                merged_lines.append(eng_sentence)
                                merged_lines.append("\n\n")
                                merged_lines.append(kor_sentence)
                                
                                if time_judge:  # time_judgeê°€ Noneì´ ì•„ë‹ ë•Œ
                                    time_str = time_judge.group(0) 
                                    for j in range(len(adw_script)):
                                        if time_str in adw_script[j]:  # time_strì´ adw[j]ì— ìˆëŠ”ì§€ í™•ì¸
                                            merged_lines.append("\n")
                                            merged_lines.append(adw_script[j].replace(time_str,""))
                                                
                                    merged_lines.append("\n\n")      
                                else:
                                    merged_lines.append("\n")
                              
                            merged_en_ko_script = "".join(merged_lines)

                                
                            merged_en_ko_script_split = merged_en_ko_script.splitlines()
                                 
                                        
                            word_file_shadowing_script = create_word_file_shadow_script(merged_en_ko_script_split,title_video,learn_code,want_font,native_font,font_size)
                                # ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            
                            st.download_button(
                                            label="Download Subtitles I Want to Learn.docx",
                                            data=word_file,
                                            file_name="Want_Learn_language.docx",
                                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                            key="download_button_1"
                                )
                            st.download_button(
                                            label="Download Advanced Language.docx",
                                            data=word_file_adw,
                                            file_name="Advanced Language.docx",
                                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                            key="download_button_word_file_adw"
                                )
                            
                            st.download_button(
                                    label="Download Shadowing File_Analysis Version.docx",
                                    data=word_file_shadowing_script ,
                                    file_name="Shadowing File_Analysis.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                    key="download_button_merged_shadowing_script"
                                    )            
                            st.success("I've completed it! Expand your world!", icon="âœ…")
                            #display_chat_message("assistant","I've completed it! Expand your world!")
                            st.balloons()
                


            except Exception as e:
                # list_available_languagesì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì²˜ë¦¬
                display_chat_message("assistant", f"An error occurred while loading the subtitle language list:{e}")
    except Exception as e:
        # transcript_list ì´ˆê¸°í™”ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì²˜ë¦¬
        display_chat_message("assistant", f"An error occurred while loading the subtitle language list:{e}")

